# ego_centric_vision
tools and data for analyzing and detecting actions in ego centric videos


Annotations for 'Learning to Localize and Align Fine-Grained Actions to Sparse Instructions'
---------------
* The annotations that were used in the 'Learning to Localize and Align Fine-Grained Actions to Sparse Instructions' paper are contained in the data folder
* The files contain EDGTEA, GTEA_Gaze and BEOID. The folders contain the cleaned step by step recipes and the action to recipe step annotations. 

Hand Masks
----------
the hand masks used in the 'Learning to Localize and Align Fine-Grained Actions to Sparse Instructions' paper.
go to the hand_mask directory
edit the run_hands.sh for the video root directory, which video you want and the directory for where you want the outputs to go

* A pre-trained hand model (on GTEA Gaze+) can be downloaded from [this link](https://dl.dropboxusercontent.com/u/39491694/hand_vgg16_iter_36000.caffemodel)

Action Datasets
---------------
the file action_datasets.md contains a list of the most common action datasets both for First Person Vision (FPV) and 3rd person vision



